{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Seaborn and matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Random seed for reproducibility\n",
    "seed = 202\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Garbage collector\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataframes without JSON features and useless features\n",
    "train_raw_df = pd.read_csv(\"(2)cleaned_train.csv\",\n",
    "    dtype={'fullVisitorId': str}, nrows=None)\n",
    "test_raw_df = pd.read_csv(\"(2)cleaned_test.csv\",\n",
    "    dtype={'fullVisitorId': str}, nrows=None)\n",
    "train_raw_df.shape, test_raw_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: the next step is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect disguised nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check how many categories are present per column (before nans substitution)\n",
    "for c in train_raw_df.columns:\n",
    "    print(c, len(np.unique(train_raw_df[c].astype(str))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(train_raw_df['totals_sessionQualityDim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "POSSIBLE DATA HANDLING\n",
    "\n",
    "device_browser:\n",
    "    'Safari (in-app)' -> 'Safari'\n",
    "    'Opera Mini' -> 'Opera'\n",
    "    'Mozilla' -> 'Firefox'\n",
    "    'Mozilla Compatible Agent' -> 'Firefox'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_raw_df['totals_timeOnSite'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert disguised nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list = [\n",
    "    \"(not set)\",\n",
    "    \"not available in demo dataset\",\n",
    "    \"not.configured\",\n",
    "    \"(not provided)\",\n",
    "    \"unknown.unknown\",\n",
    "    \"/\",\n",
    "    \"(Other)\",\n",
    "    \"(other)\",\n",
    "    \"(none)\",\n",
    "    \"nan\",\n",
    "    \"[]\"\n",
    "]\n",
    "\n",
    "nan_dict = {nl:np.nan for nl in nan_list}\n",
    "\n",
    "# convert all \"disguised\" missing values to nans\n",
    "def miss_to_nan(df):\n",
    "    df.replace(nan_dict, inplace=True) # convert disguised NaNs\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the preprocessing up to this point and save into copies\n",
    "train_df = miss_to_nan(train_raw_df)\n",
    "test_df = miss_to_nan(test_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: the next step is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the nan condition in totals.newVisits\n",
    "from itertools import compress\n",
    "sum(list(compress(train_df['visitNumber'] > 1, train_df['totals.newVisits'].isnull()))) == train_df['totals.newVisits'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the nan condition in totals.newVisits\n",
    "print(sum(train_df['totals.newVisits'].isnull()))\n",
    "sum(list(compress(train_df['totals.newVisits'].isnull(), train_df['visitNumber'] > 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is difference between nan and (none) in df['trafficSource.medium']\n",
    "print(sum(train_df['trafficSource.medium'].isnull()))\n",
    "sum(list(compress(train_df['trafficSource.medium'].isnull(), train_df['totals.transactionRevenue'] == 0)))\n",
    "\n",
    "### c'Ã¨ un valore con revenue !!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same shit for trafficSource.source\n",
    "print(sum(train_df['trafficSource.source'].isnull()))\n",
    "sum(list(compress(train_df['trafficSource.source'].isnull(), train_df['totals.transactionRevenue'] == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check again number of categories in each variable (eg column)\n",
    "for c in train_df.columns:\n",
    "    print(c, len(np.unique(train_df[c].astype(str))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_df.columns:    \n",
    "    if len(train_df[i].unique()) <= 100:\n",
    "        print(i + '\\n')\n",
    "        print(train_df[i].unique())\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: the next step is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_distribution(col, dtype='float64'):\n",
    "    \"\"\"Plot a single numerical column distribution in linear and log scale.\"\"\"\n",
    "    fig, axis = plt.subplots(1, 2, figsize=(14,6))\n",
    "    axis[0].set_title(\"Linear scale\")\n",
    "    axis[1].set_title(\"Log scale\")\n",
    "    \n",
    "    train_df[col], test_df[col] = train_df[col].astype(dtype), test_df[col].astype(dtype)\n",
    "    ax1 = sns.kdeplot(train_df[col].dropna(), label='train', ax=axis[0])\n",
    "    ax2 = sns.kdeplot(test_df[col].dropna(), label='test', ax=axis[0])\n",
    "    ax3 = sns.kdeplot(np.log(train_df[col].dropna()), label='train', ax=axis[1])\n",
    "    ax4 = sns.kdeplot(np.log(test_df[col].dropna()), label='test', ax=axis[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_distribution('totals_transactionRevenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nans(df):\n",
    "    \n",
    "    \n",
    "    df['channelGrouping'].fillna('Organic Search', inplace=True) # few nans, a lot of \"Organic Search\"\n",
    "    df['device_browser'].fillna('Chrome', inplace=True) # few nans, a lot of \"Chrome\"\n",
    "    df['device_operatingSystem'].fillna('unknown', inplace=True) # no predominance of any value, quite a bit of nans, keep separated category\n",
    "    df['totals_pageviews'].fillna(1, inplace=True) # many 1s, nans do not bring any revenue -> nans become 1s\n",
    "    df['trafficSource_medium'].fillna('unknown', inplace=True) # seems to be the same\n",
    "    df['trafficSource_source'].fillna('unknown', inplace = True) # seems to be the same\n",
    "    df['customDimensions'].fillna('unknown', inplace = True)\n",
    "    \n",
    "    \n",
    "    df['geoNetwork_continent'].fillna('unknown', inplace=True)# keep separate category\n",
    "    df['geoNetwork_country'].fillna('unknown', inplace=True)# keep separate category\n",
    "    df['geoNetwork_subContinent'].fillna('unknown', inplace=True)# keep separate category\n",
    "\n",
    "    # Targets\n",
    "    df['totals_transactionRevenue'].fillna(0, inplace=True)\n",
    "    df['totals_transactions'].fillna(0, inplace=True)\n",
    "    df['totals_totalTransactionRevenue'].fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Features with huge amout of nans\n",
    "    df['totals_timeOnSite'].fillna(0, inplace=True) # to be decided together\n",
    "    df['totals_sessionQualityDim'].fillna(0, inplace=True) # 1 means bad quality, and is the most frequent value. Anyway putting 0 may be significative\n",
    "    df['geoNetwork_city'].fillna('unknown', inplace=True)\n",
    "    df['geoNetwork_metro'].fillna('unknown', inplace=True)\n",
    "    df['geoNetwork_networkDomain'].fillna('unknown', inplace=True)\n",
    "    df['geoNetwork_region'].fillna('unknown', inplace=True)\n",
    "    df['totals_bounces'].fillna(0, inplace=True) # nan and 1\n",
    "    df['trafficSource_adContent'].fillna('unknown', inplace=True)\n",
    "    df['trafficSource_adwordsClickInfo.adNetworkType'].fillna('unknown', inplace=True)\n",
    "    df['trafficSource_adwordsClickInfo.gclId'].fillna('unknown', inplace=True)\n",
    "    df['trafficSource_adwordsClickInfo.isVideoAd'].fillna(True, inplace=True) # nan and False\n",
    "    df['trafficSource_adwordsClickInfo.page'].fillna(0, inplace=True) # 0 is not cointained in the set\n",
    "    df['trafficSource_adwordsClickInfo.slot'].fillna('unknown', inplace=True)\n",
    "    df['trafficSource_campaign'].fillna('unknown', inplace=True)\n",
    "    df['trafficSource_isTrueDirect'].fillna(False, inplace=True) # nan and True\n",
    "    df['trafficSource_keyword'].fillna('unknown', inplace=True)\n",
    "    df['trafficSource_referralPath'].fillna('unknown', inplace=True)\n",
    "    df['totals_newVisits'].fillna(0, inplace=True)# totals.newVisits is always == nan when visitNumber > 1, we keep it for now but it will be dropped eventually\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = fill_nans(train_df)\n",
    "test_df = fill_nans(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast numerical features to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float_features = ['visitId', 'visitNumber', 'visitStartTime', 'totals.bounces', 'totals.hits',\n",
    "#                  'totals.newVisits', 'totals.pageviews', 'trafficSource.adwordsClickInfo.page',\n",
    "#                  'trafficSource.isTrueDirect']\n",
    "\n",
    "# for col in float_features:\n",
    "#     train_df[col] = train_df[col].astype(float)\n",
    "#     test_df[col] = test_df[col].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast string categorical features to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string_features = ['channelGrouping', 'customDimensions', 'device_browser', 'device_deviceCategory',\n",
    "#                   'device_operatingSystem', 'geoNetwork_city', 'geoNetwork_continent', 'geoNetwork_country',\n",
    "#                   'geoNetwork_metro', 'geoNetwork_networkDomain', 'geoNetwork_region', 'geoNetwork_subContinent',\n",
    "#                   'trafficSource_adContent', 'trafficSource_adwordsClickInfo.adNetworkType', 'trafficSource_adwordsClickInfo.gclId',\n",
    "#                   'trafficSource_adwordsClickInfo.slot', 'trafficSource_campaign', 'trafficSource_referralPath',\n",
    "#                   'trafficSource_medium', 'trafficSource_source', 'trafficSource_keyword']\n",
    "\n",
    "# for col in string_features:\n",
    "#     train_df[col] = train_df[col].str.lower().astype(str)\n",
    "#     train_df[col] = train_df[col].astype(str)\n",
    "#     test_df[col] = test_df[col].str.lower().astype(str)\n",
    "#     test_df[col] = test_df[col].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: the next step is optional (may improve performances but it has to be tested)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning rare categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually we have no longer missing values, and we can manage the problem of the rare categories\n",
    "# We group them in an overfeature\n",
    "def clear_rare_categories(df, feature, limit = 900):\n",
    "\n",
    "    vc = df[feature].value_counts()\n",
    "    \n",
    "    common = vc > limit\n",
    "    common = set(common.index[common].values)\n",
    "    print(\"Set\", sum(vc <= limit), feature, \"categories to 'other';\", end=\" \")\n",
    "    \n",
    "    df.loc[df[feature].map(lambda x: x not in common), feature] = 'other'\n",
    "    print(\"now there are\", df[feature].nunique(), \"categories\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "raw_string_features = [\n",
    "    \"channelGrouping\",\n",
    "    \"device_browser\",\n",
    "    \"device_deviceCategory\",\n",
    "    \"device_operatingSystem\",\n",
    "    \"geoNetwork_city\",\n",
    "    \"geoNetwork_continent\",\n",
    "    \"geoNetwork_country\",\n",
    "    \"geoNetwork_metro\",\n",
    "    \"geoNetwork_networkDomain\",\n",
    "    \"geoNetwork_region\",\n",
    "    \"geoNetwork_subContinent\",\n",
    "    \"trafficSource_adContent\",\n",
    "    \"trafficSource_adwordsClickInfo.adNetworkType\",\n",
    "    \"trafficSource_adwordsClickInfo.slot\",\n",
    "    \"trafficSource_campaign\",\n",
    "    \"trafficSource_keyword\",\n",
    "    \"trafficSource_medium\",\n",
    "    \"trafficSource_referralPath\",\n",
    "    \"trafficSource_source\",\n",
    "    \"customDimensions\",\n",
    "    \"trafficSource_adwordsClickInfo.gclId\"\n",
    "]\n",
    "\n",
    "#check\n",
    "for i in train_df.columns:\n",
    "    if(i not in raw_string_features):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feat in raw_string_features:\n",
    "    print(\"\\nTRAIN:\\n\")\n",
    "    train_df = clear_rare_categories(train_df, feat)\n",
    "    print(\"\\nTEST:\\n\")\n",
    "    test_df = clear_rare_categories(test_df, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_df.columns:    \n",
    "    if len(train_df[i].unique()) <= 100:\n",
    "        print(i + '\\n')\n",
    "        print(train_df[i].unique())\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check again number of categories in each variable (eg column)\n",
    "for c in train_df.columns:\n",
    "    print(c, len(np.unique(train_df[c].astype(str))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop meaningless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningless_columns =[\"visitId\"]\n",
    "\n",
    "def drop_meaningless_columns(df, meaningless_columns):\n",
    "    df.drop(meaningless_columns, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_df = drop_meaningless_columns(train_df, meaningless_columns)\n",
    "test_df = drop_meaningless_columns(test_df, meaningless_columns)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: the next step is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "for col in raw_string_features:\n",
    "    print(col)\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n",
    "    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n",
    "    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('(3)preprocessed_train.csv', index = False)\n",
    "test_df.to_csv('(3)preprocessed_test.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
